[#s2,reftext=S.2]
=== (S.2) Functionality

ifdef::env-draft[]
TIP: _**This is the bulk of the System book, describing elements of functionality (behaviors)**. This chapter corresponds to the traditional view of requirements as defining "**what the system does**”. It is organized as one section, S.2.n, for each of the components identified in <<s1>>, describing the corresponding behaviors (functional and non-functional properties)._  <<BM22>>
endif::[]

The SocialEyes-based system operates in two primary modes: a recording mode for offline data collection and post-hoc analysis, and a streaming mode for real-time data processing and visualization. Each component contributes to these modes to enable synchronized multi-person gaze analytics, storage, and visualization for classroom environments.

The system’s functionality is organized according to its major components, as described in <<s1>>. Each subsection defines the principal behaviors (functional and non-functional) of that component.

.Data Flow Sequence During Classroom Session
image::socialeyes-sequence.svg[width=100%]

[#s2.1]
==== (S.2.1) Data Ingestion Module
- Collects gaze coordinates, eye images, and egoview video streams from Pupil Labs Neon devices via the Neon Companion App API.
- Operates in both recording (offline storage) and streaming (real-time transmission) modes.
- Uses Network Time Protocol (NTP) to synchronize timestamps across multiple devices for accurate multi-stream alignment.
- Supports ingestion of a central camera feed (centralview) to provide a shared classroom scene for homography-based mapping.
- Employs Kafka producers for low-latency data streaming and fault-tolerant buffering in real-time mode.
- Detects and recovers from connection issues such as device dropouts or packet loss.
// - [TBD with supervisors: Confirm need for integration with Central Camera feed and multi-device scaling for initial proof-of-concept.]

[#s2.2]
==== (S.2.2) Data Processing & Analytics
- Performs data filtering (noise reduction, blink removal) and calibration corrections on incoming gaze and video streams.
- Executes homography-based gaze projection, mapping each viewer’s egoview gaze data onto the shared central camera view.
- Computes both real-time and post-session metrics, including gaze velocity, entropy, heatmap similarity, and normalized contour area to quantify attention and engagement.
- Generates heatmaps, visual overlays, and summary statistics for collective gaze behavior analysis.
- Balances computational efficiency and accuracy through lightweight algorithms suitable for classroom-scale use.
// - [TBD with supervisors: Define privacy/anonymization requirements (e.g., masking faces or anonymizing device identifiers).]

[#s2.3]
==== (S.2.3) Backend Services
- Hosts REST/GraphQL APIs (via Flask) for dashboard communication, session management, and data retrieval.
- Manages session lifecycle operations such as start, stop, and resume.
- Integrates with Kafka consumers to process real-time streams and forward processed results to the dashboard and analytics modules.
- Enforces authentication and authorization for all API requests and role-based access.
- Provides a Flask-based monitoring interface for real-time verification of data integrity, device status, and stream quality.
- Supports automated build, linting, and test pipelines through GitHub Actions CI/CD for continuous delivery.
// - [TBD with supervisors: Confirm scope of external integrations (e.g., Pupil Cloud synchronization, LMS links) in Rev 0 vs later milestones.]

[#s2.4]
==== (S.2.4) Database / Storage
- Stores synchronized gaze data, egoview and centralview recordings, and derived analytics outputs.
- Supports both local databases (SQLite or PostgreSQL) for development and remote storage (Pupil Cloud) for post-processing.
- Enables real-time data access for visualization while maintaining persistence for post-session replay and analysis.
- Implements configurable retention and anonymization policies to protect participant data and comply with ethical guidelines.
// - [TBD with supervisors: Determine whether long-term archival or only short-term proof-of-concept storage is required.]

[#s2.5]
==== (S.2.5) Instructor Dashboard (Frontend)
- Provides an intuitive React + TypeScript interface for instructors and researchers to visualize gaze analytics.
- Displays real-time heatmaps, focus indicators, and group engagement metrics generated by the Analytics module.
- Presents post-session summaries and trend reports for comparative study of classroom attention patterns.
- Supports role-based access control and secure login consistent with backend authentication.
- Allows data and report export (e.g., CSV, JSON, or graphical formats) for documentation and further research.
// - [TBD with supervisors: Confirm whether LMS integration (e.g., Avenue to Learn) or external data export is required at this stage.]

[#s2.6]
==== (S.2.6) Supporting Infrastructure
- Authentication & Access Control: Implements secure login, session tokens, and role-based user management for instructors and researchers.
- Error Logging & Monitoring: Collects system health metrics, stream latency, and runtime errors using integrated Grafana/Prometheus dashboards.
- Time Synchronization Service (NTP): Ensures uniform timestamps across all devices, enabling consistent temporal alignment in analytics.
- Deployment / Runtime Environment: Utilizes Docker containers for reproducibility, isolated builds, and cross-platform deployment.
- Continuous Integration (CI/CD): Automates testing, linting, and deployment workflows using GitHub Actions to maintain code quality.
// - [Optional Simulation/Test Harness] – TBD with supervisors: develop replay utilities to simulate recorded sessions for verification and future testing.