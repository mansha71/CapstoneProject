[#s2,reftext=S.2]
=== (S.2) Functionality

ifdef::env-draft[]
TIP: _**This is the bulk of the System book, describing elements of functionality (behaviors)**. This chapter corresponds to the traditional view of requirements as defining "**what the system does**”. It is organized as one section, S.2.n, for each of the components identified in <<s1>>, describing the corresponding behaviors (functional and non-functional properties)._  <<BM22>>
endif::[]

The SocialEyes-based system operates in two primary modes: a recording mode for offline data collection and post-hoc analysis, and a streaming mode for real-time data processing and visualization. Each component contributes to these modes to enable synchronized multi-person gaze analytics, storage, and visualization for classroom environments.

The system’s functionality is organized according to its major components, as described in <<s1>>. Each subsection defines the principal behaviors (functional and non-functional) of that component.

==== Functional Requirements

The following tables enumerate the functional requirements (FRs) of the SocialEyes system.  
Requirements are categorized according to implementation phase:

* **Rev 0 Functional Requirements** – essential capabilities forming the proof-of-concept deliverable (single-device ingestion, basic analytics, and dashboard visualization).  
* **Stretch / Future Functional Requirements** – enhancements planned for Rev 1 or later (multi-device scaling, additional roles, and extended analytics).

Each requirement corresponds to one or more components described in <<s1>> and detailed in sections S.2.1–S.2.6.

[cols="1,4",options="header"]
|===
| ID | Rev 0 Functional Requirements

| FR-1 | The system shall support two operational modes: (a) Recording mode for offline data capture and post-hoc analysis, and (b) Streaming mode for real-time data processing and visualization.
| FR-2 | The system shall allow an instructor to initiate, pause, resume, and terminate classroom recording sessions via the dashboard or command-line interface.
| FR-3 | The system shall connect to a Pupil Labs Neon device through the Neon Companion App API and collect synchronized gaze, eye, and egoview video data.
| FR-5 | The system shall synchronize incoming data streams using Network Time Protocol (NTP) to maintain consistent timestamps.
| FR-6 | The system shall recover from transient connection losses by automatically resuming data collection within 5 seconds of detection.
| FR-7 | The system shall perform data filtering and calibration correction to remove noise, blinks, and miscalibrations in gaze and video streams.
| FR-8 | The system shall apply homography-based gaze mapping to project gaze points from the egoview to the shared central coordinate space.
| FR-9 | The system shall compute real-time engagement and attention metrics, including gaze velocity, entropy, and normalized contour area.
| FR-10 | The system shall generate post-session summary analytics, including heatmaps, time-series graphs, and participant-level statistics.
| FR-11 | The system shall store raw and processed data in a local database (SQLite or PostgreSQL) and support optional upload to Pupil Cloud.
| FR-12 | The system shall ensure that each session includes metadata such as timestamp, participant ID, and session ID.
| FR-13 | The system shall expose REST or GraphQL APIs (via Flask) for analytics retrieval and session management.
| FR-14 | The system shall enforce role-based authentication and authorization, permitting access only to approved users (e.g., instructor).
| FR-15 | The dashboard shall display real-time visual analytics, including heatmaps, attention indicators, and engagement metrics.
| FR-16 | The dashboard shall allow users to export post-session analytics and visualizations in common formats (CSV, JSON, PNG).
| FR-18 | The system shall log runtime events and performance metrics in a persistent monitoring interface (e.g., Prometheus/Grafana).
| FR-19 | The system shall employ Dockerized containers for consistent and reproducible deployment across platforms.
| FR-20 | The system shall include continuous integration (CI/CD) pipelines to automatically execute build, lint, and test workflows prior to deployment.
| FR-22 | The system shall maintain traceability of session data, linking raw data, analytics, and reports via unique session identifiers.
| FR-23 | The system shall support basic configuration options (e.g., device ID, data directory, export format) accessible via configuration files or dashboard settings.
| FR-24 | The system shall allow replay of recorded sessions to verify synchronization accuracy and re-run analytics for validation purposes.
|===

[cols="1,4",options="header"]
|===
| ID | Stretch / Future Functional Requirements

| FR-4 | The system shall optionally integrate a Central Camera feed to capture a shared classroom view (centralview) for multi-person gaze alignment.
| FR-17 | The system shall provide separate role-based views for instructors and researchers, limiting access according to role permissions.
| FR-21 | The system shall implement privacy and anonymization features, including masking of identifiable faces or eye images in exported visualizations.
| FR-25 | The system shall support scaling to multiple Pupil Labs Neon devices (target ≥ 10) during live streaming, maintaining inter-device synchronization accuracy within ± 20 ms, consistent with benchmarks established in the SocialEyes reference framework [subject to validation during Rev 1 testing.]
|===

.Data Flow Sequence During Classroom Session
image::socialeyes-sequence.svg[width=100%, alt="Data Flow Sequence During Classroom Session"]
This sequence diagram illustrates the flow of data during both recording (offline) and streaming (real-time) modes, showing how gaze, video, and analytics information move between the system’s major components.


[#s2.1]
==== (S.2.1) Data Ingestion Module
- Collects gaze coordinates, eye images, and egoview video streams from Pupil Labs Neon devices via the Neon Companion App API.
- Operates in both recording (offline storage) and streaming (real-time transmission) modes.
- Uses Network Time Protocol (NTP) to synchronize timestamps across multiple devices for accurate multi-stream alignment.
- Supports ingestion of a central camera feed (centralview) to provide a shared classroom scene for homography-based mapping.
- Employs Kafka producers for low-latency data streaming and fault-tolerant buffering in real-time mode.
- Detects and recovers from connection issues such as device dropouts or packet loss.
// - [TBD with supervisors: Confirm need for integration with Central Camera feed and multi-device scaling for initial proof-of-concept.]

[#s2.2]
==== (S.2.2) Data Processing & Analytics
- Performs data filtering (noise reduction, blink removal) and calibration corrections on incoming gaze and video streams.
- Executes homography-based gaze projection, mapping each viewer’s egoview gaze data onto the shared central camera view.
- Computes both real-time and post-session metrics, including gaze velocity, entropy, heatmap similarity, and normalized contour area to quantify attention and engagement.
- Generates heatmaps, visual overlays, and summary statistics for collective gaze behavior analysis.
- Balances computational efficiency and accuracy through lightweight algorithms suitable for classroom-scale use.
// - [TBD with supervisors: Define privacy/anonymization requirements (e.g., masking faces or anonymizing device identifiers).]

[#s2.3]
==== (S.2.3) Backend Services
- Hosts REST/GraphQL APIs (via Flask) for dashboard communication, session management, and data retrieval.
- Manages session lifecycle operations such as start, stop, and resume.
- Integrates with Kafka consumers to process real-time streams and forward processed results to the dashboard and analytics modules.
- Enforces authentication and authorization for all API requests and role-based access.
- Provides a Flask-based monitoring interface for real-time verification of data integrity, device status, and stream quality.
- Supports automated build, linting, and test pipelines through GitHub Actions CI/CD for continuous delivery.
// - [TBD with supervisors: Confirm scope of external integrations (e.g., Pupil Cloud synchronization, LMS links) in Rev 0 vs later milestones.]

[#s2.4]
==== (S.2.4) Database / Storage
- Stores synchronized gaze data, egoview and centralview recordings, and derived analytics outputs.
- Supports both local databases (SQLite or PostgreSQL) for development and remote storage (Pupil Cloud) for post-processing.
- Enables real-time data access for visualization while maintaining persistence for post-session replay and analysis.
- Implements configurable retention and anonymization policies to protect participant data and comply with ethical guidelines.
// - [TBD with supervisors: Determine whether long-term archival or only short-term proof-of-concept storage is required.]

[#s2.5]
==== (S.2.5) Instructor Dashboard (Frontend)
- Provides an intuitive React + TypeScript interface for instructors and researchers to visualize gaze analytics.
- Displays real-time heatmaps, focus indicators, and group engagement metrics generated by the Analytics module.
- Presents post-session summaries and trend reports for comparative study of classroom attention patterns.
- Supports role-based access control and secure login consistent with backend authentication.
- Allows data and report export (e.g., CSV, JSON, or graphical formats) for documentation and further research.
// - [TBD with supervisors: Confirm whether LMS integration (e.g., Avenue to Learn) or external data export is required at this stage.]

[#s2.6]
==== (S.2.6) Supporting Infrastructure
- Authentication & Access Control: Implements secure login, session tokens, and role-based user management for instructors and researchers.
- Error Logging & Monitoring: Collects system health metrics, stream latency, and runtime errors using integrated Grafana/Prometheus dashboards.
- Time Synchronization Service (NTP): Ensures uniform timestamps across all devices, enabling consistent temporal alignment in analytics.
- Deployment / Runtime Environment: Utilizes Docker containers for reproducibility, isolated builds, and cross-platform deployment.
- Continuous Integration (CI/CD): Automates testing, linting, and deployment workflows using GitHub Actions to maintain code quality.
// - [Optional Simulation/Test Harness] – TBD with supervisors: develop replay utilities to simulate recorded sessions for verification and future testing.